\documentclass[11pt]{article}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{geometry}
\geometry{margin=1in}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}

\title{Numerical Analysis of the Golub-Kahan SVD Algorithm:\\Experiments in Bidiagonalization, Singular Value Computation,\\and Modern Randomized Extensions}
\author{Mathematical Computing Laboratory}
\date{\today}

\begin{document}

\maketitle

\section{Experiment}

\subsection{Experimental Setup and Methodology}

To ensure controlled, reproducible experiments, our test matrices were constructed using explicit singular value decomposition: given a desired sequence of singular values $\{\sigma_i\}$, we generate random orthogonal matrices $U$ and $V$ via QR decomposition of Gaussian random matrices, then form $A = U\text{diag}(\sigma_i)V^T$.

We employ several controlled singular value distributions to probe different aspects of the algorithms:
\begin{itemize}
\item \textbf{Geometric decay:} $\sigma_i = \kappa^{-i/n}$ for condition number $\kappa$, simulating typical data matrices.
\item \textbf{Uniform decay:} $\sigma_i = 1 - (i-1)/(n-1)(1 - 1/\kappa)$, providing evenly spaced singular values.
\end{itemize}

Error metrics include the relative Frobenius norm $\|A - \hat{A}\|_F / \|A\|_F$ for reconstruction accuracy, the orthogonality defect $\|U^TU - I\|_F$ for numerical stability, and relative singular value error $\|\sigma - \hat{\sigma}\|_2 / \|\sigma\|_2$.

\subsection{Experiment 1: Validation of the Golub-Kahan Implementation}

Our first experiment validates that the Householder-based bidiagonalization algorithm can be implemented to achieve accuracy comparable to highly optimized LAPACK routines.

\subsubsection{Experimental Design}

We tested \footnote{Github:\url{https://github.com/bingyulab/paper-replication}} square matrices of sizes $n \in \{50, 100, 200, 500, 1000, 2000\}$ with condition number $\kappa(A) = 10^4$, comparing our implementation against SciPy's \texttt{svd} function (which calls LAPACK's \texttt{dgesvd}). For each matrix, we measured:
\begin{enumerate}
\item Computation time for the complete SVD.
\item Reconstruction error $\|A - U\Sigma V^T\|_F / \|A\|_F$.
\item Singular value accuracy relative to SciPy/LAPACK.
\item Orthogonality of computed $U$ and $V$ matrices.
\end{enumerate}

\subsubsection{Results and Analysis}

Figure~\ref{fig:exp1} presents the comparative results. The left panel shows that our Golub-Kahan implementation scales as $O(n^3)$, consistent with the theoretical complexity, but is approximately 81 times slower than the highly optimized LAPACK implementation for $n=2000$. The Numba-accelerated implementation, despite just-in-time (JIT) compilation to optimized machine code, demonstrates inconsistent performance relative to pure Python. For small matrices ($n=50$–$100$), Numba provides roughly 3–5$\times$ speedup; the benefit drops to $\sim$1.7$\times$ at $n=200$ and $\sim$1.5$\times$ at $n=500$. For larger matrices ($n \geq 1000$), Numba becomes slower than the unoptimized Python implementation (e.g., Numba is $\approx$1.48$\times$ slower than our Python implementation at $n=2000$).

\begin{figure}[t]
\centering
\includegraphics[width=\textwidth]{../imgs/svd/exp1_validation.pdf}
\caption{Validation of the Golub-Kahan SVD implementation. Left: Computation time comparison showing expected $O(n^3)$ scaling. Center: Reconstruction accuracy demonstrating that both implementations achieve machine precision. Right: Orthogonality errors remaining at $\sim 10^{-13}$ even for $n=2000$, confirming numerical stability.}
\label{fig:exp1}
\end{figure}

This phenomenon stems from fundamental differences in how Python+NumPy and Numba handle memory operations. NumPy operations are implemented in highly optimized C code that uses BLAS (Basic Linear Algebra Subprograms) for matrix operations. BLAS implementations (e.g., OpenBLAS, Intel MKL) employ sophisticated cache-blocking strategies. NumPy's use of multi-threaded BLAS (via OpenMP) can saturate memory bandwidth more effectively than Numba's single-threaded loops. SciPy SVD, using the more efficient divide-and-conquer approach `gesdd', further outperforms both implementations.

The center panel reveals the critical finding: our reconstruction errors range from $1.18 \times 10^{-15}$ for $n=50$ to $3.75 \times 10^{-15}$ for $n=2000$, essentially at machine precision ($\epsilon_{\text{mach}} \approx 2.22 \times 10^{-16}$ for double precision). These values are indistinguishable from those achieved by SciPy, confirming that our implementation is correct. The slight increase with $n$ is consistent with the expected accumulation of $O(n)$ rounding errors during the $O(n)$ transformations.

The right panel demonstrates orthogonality preservation: the computed $U$ and $V$ matrices satisfy $\|U^TU - I\|_F \approx 10^{-13}$ and $\|V^TV - I\|_F \approx 10^{-13}$ even for $n=2000$. This is well within the bounds predicted, which suggests orthogonality defects of order $n\epsilon_{\text{mach}}$. For $n=2000$, we have $n\epsilon_{\text{mach}} \approx 4.4 \times 10^{-13}$, in excellent agreement with the observed values of $\sim 2 \times 10^{-13}$. There is slight increase comparing with SciPy, likely due to LAPACK's additional optimization.

\subsubsection{Theoretical Interpretation}

The success of the Golub-Kahan algorithm can be understood through the lens of backward stability. Each Householder transformation $H = I - 2ww^T$ is exactly orthogonal, so the product $P = H_1H_2\cdots H_k$ is also exactly orthogonal in exact arithmetic. In floating-point arithmetic, we compute $\tilde{P}$ satisfying $\tilde{P}^T\tilde{P} = I + E$ where $\|E\|_F = O(k\epsilon_{\text{mach}})$. Since $k = O(n)$, this explains the observed orthogonality errors.

For the reconstruction error, the key inequality is
\begin{equation}
\|A - \tilde{U}\tilde{\Sigma}\tilde{V}^T\|_F \leq \|A\|_F \cdot O(n\epsilon_{\text{mach}}),
\end{equation}
which our experiments confirm with the constant hidden in $O(\cdot)$ being approximately 1-2.

\subsection{Experiment 2: Accuracy of Singular Value Computation Methods}

Having validated the bidiagonalization phase, we now examine the critical question posed in the Golub-Kahan paper: how accurately can singular values be extracted from the bidiagonal matrix, particularly when the matrix is ill-conditioned?

\subsubsection{Experimental Design}

We constructed $100 \times 100$ matrices with geometric singular value decay for condition numbers $\kappa \in \{10^2, 10^4, 10^6, 10^8, 10^{10}, 10^{12}\}$. After bidiagonalization, we computed singular values using both the $J^TJ$ method (forming the $n \times n$ tridiagonal symmetric matrix and computing its eigenvalues) and the $2n \times 2n$ method (forming the augmented skew-symmetric matrix). We compared both against ground truth singular values computed by applying LAPACK's SVD directly to the original matrix $A$.

\subsubsection{Results and Analysis}

Figure~\ref{fig:exp2} presents the comparative accuracy of the two methods. The left panel shows overall relative error in the singular value vector, while the right panel focuses specifically on the smallest singular value $\sigma_{\min}$, which is most susceptible to numerical errors.

\begin{figure}[t]
\centering
\includegraphics[width=\textwidth]{../imgs/svd/exp2_sv_accuracy.pdf}
\caption{Comparison of singular value computation methods for ill-conditioned matrices. Left: Overall accuracy showing that the $2n \times 2n$ method maintains machine precision across all condition numbers. Right: Smallest singular value accuracy revealing the catastrophic failure of the $J^TJ$ method for $\kappa > 10^8$.}
\label{fig:exp2}
\end{figure}

For well-conditioned matrices ($\kappa \leq 10^4$), both methods achieve comparable accuracy, with errors at or below $10^{-12}$. However, as conditioning deteriorates, a dramatic divergence emerges. This behavior can be explained through condition number analysis. When we form $K = J^TJ$, we are implicitly squaring the singular values: the eigenvalues of $K$ are $\sigma_i^2$. The condition number of the eigenvalue problem for $K$ is therefore
\begin{equation}
\kappa(K) = \frac{\lambda_{\max}}{\lambda_{\min}} = \frac{\sigma_{\max}^2}{\sigma_{\min}^2} = \kappa(J)^2.
\end{equation}

Thus, an $O(\epsilon_{\text{mach}})$ perturbation in computing $K$ leads to an $O(\kappa^2 \epsilon_{\text{mach}})$ error in $\lambda_{\min}$, and consequently an $O(\kappa \sqrt{\epsilon_{\text{mach}}})$ error in $\sigma_{\min}$ after taking the square root. For $\kappa = 10^{10}$, this predicts $\sigma_{\min}$ errors of order $10^{10} \times 10^{-8} = 10^2$, which is consistent with complete loss of accuracy.

In contrast, the $2n \times 2n$ formulation avoids squaring. The eigenvalue problem for $T = \begin{pmatrix} 0 & J \\ J^T & 0 \end{pmatrix}$ has condition number $\kappa(T) = \kappa(J)$, leading to $O(\kappa \epsilon_{\text{mach}})$ errors in the computed singular values. This is a dramatic improvement: linear rather than quadratic dependence on $\kappa$.

\subsection{Experiment 3: Robustness of Pseudo-Inverse Computation}

The practical utility of the SVD lies not merely in computing singular values but in solving least-squares problems via the pseudo-inverse. Section 5 of the Golub-Kahan paper emphasizes that the SVD-based pseudo-inverse (i.e. $A^+ = V\Sigma^+ U^T$) is numerically superior to the normal equations approach, particularly for ill-conditioned or rank-deficient systems.

\subsection{Experimental Design}

We tested three $50 \times 30$ matrices representing different pathological cases:
\begin{enumerate}
\item \textbf{Well-conditioned:} $\kappa(A) = 100$, full rank.
\item \textbf{Ill-conditioned:} $\kappa(A) = 10^8$, full rank but numerically sensitive.
\item \textbf{Rank-deficient:} True rank $r=15$, with $\sigma_{16} = \cdots = \sigma_{30} = 0$.
\end{enumerate}

For each matrix, we generated a random right-hand side $b \in \mathbb{R}^{50}$ and solved the least-squares problem $\min_x \|Ax - b\|_2$ using two methods:
\begin{itemize}
\item \textbf{SVD method:} Compute $A^+$ via pseudo-inverse with threshold $\tau = 10^{-15} \sigma_{\max}$, then $x = A^+b$.
\item \textbf{Normal equations:} Solve $(A^TA)x = A^Tb$ directly via matrix inversion.
\end{itemize}

We measured both the residual norm $\|Ax - b\|_2$ (which all least-squares solutions minimize equally) and the solution norm $\|x\|_2$ (which the pseudo-inverse uniquely minimizes).

\subsubsection{Results and Analysis}

Figure~\ref{fig:exp3} presents the comparative performance. For the well-conditioned case, both methods succeed identically: residual $\approx 5.14$ and solution norm $\approx 169$, demonstrating that when $A^TA$ is well-conditioned, the simpler normal equations approach is adequate.

\begin{figure}[t]
\centering
\includegraphics[width=\textwidth]{../imgs/svd/exp3_pseudoinverse.pdf}
\caption{Comparison of pseudo-inverse computation methods for least-squares problems. Left: Residual norms showing that the normal equations catastrophically fail for rank-deficient matrices. Right: Solution norms demonstrating the minimum-norm property of the SVD-based approach.}
\label{fig:exp3}
\end{figure}

For the ill-conditioned case ($\kappa = 10^8$), both methods produce the same residual ($\approx 5.40$), but the solution norms are nearly identical: $9.52 \times 10^7$ for SVD versus $9.89 \times 10^7$ for normal equations—a difference of only 3.9\%. This near-identical performance for the ill-conditioned case appears surprising given $\kappa = 10^8$. The explanation lies in both the structure of the randomly generated $b$ and the fact that the matrix is still full rank. When $b$ lies primarily in the span of the dominant singular vectors, even extreme conditioning has limited impact on the least-squares solution. This highlights an important point: condition number alone does not determine difficulty-the interaction between $A$ and $b$ matters. 

The dramatic failure occurs for the rank-deficient matrix. Here, the SVD method correctly identifies the rank deficiency (via thresholding singular values below $10^{-15} \sigma_{\max}$) and produces a solution with residual 5.70 and norm 0.0855. The normal equations, however, attempt to invert the singular $A^TA$ matrix, yielding a solution with residual 62.5 (nearly 10 times larger) and norm 7.84 (over 90 times larger). The huge solution norm indicates that the normal equations are attempting to fit noise in directions where $A$ has no support, leading to wild oscillations.

\subsubsection{Theoretical Foundation}

The failure of normal equations for the rank-deficient case can be understood algebraically. When $A$ has rank $r < n$, the matrix $A^TA$ has a $(n-r)$-dimensional null space. Any solution to $(A^TA)x = A^Tb$ is of the form $x = x_{\text{particular}} + x_{\text{null}}$ where $x_{\text{null}} \in \text{null}(A^TA)$. Direct matrix inversion via Gaussian elimination will produce some particular solution, but which one is determined by the details of the elimination (pivoting strategy, rounding errors), not by any optimization principle. In our experiment, the computed solution had enormous norm, indicating large components in $\text{null}(A)$.

In contrast, the SVD method explicitly projects onto the orthogonal complement of the null space:
\begin{equation}
A^+ = \sum_{i: \sigma_i > \tau} \frac{1}{\sigma_i} v_i u_i^T,
\end{equation}
ensuring that $x = A^+b$ lies entirely in the row space of $A$, which is precisely the set of minimum-norm least-squares solutions.

\subsubsection{Regularization for Rank-Deficient Problems}
While Experiment 3 demonstrated the failure of normal equations for rank-deficient matrices, we can rescue this approach through regularization, trading some numerical instability for algorithmic simplicity.

The regularized normal equations (Tikhonov regularization) are
\begin{equation}
    (A^T A + \lambda I) x = A^T b,\qquad \lambda>0.
\end{equation}
The regularization parameter $\lambda>0$ improves conditioning; for example one can bound the condition number by
\begin{equation}
    \kappa(A^T A + \lambda I) \le \frac{\sigma_{\max}^2 + \lambda}{\lambda},
\end{equation}
where $\sigma_{\max}$ is the largest singular value of $A$.  For a rank-deficient $A$ (so $\sigma_{\min}=0$) the unregularized system has $\kappa=\infty$, whereas choosing $\lambda=10^{-6}$ yields roughly $\kappa\approx 10^6$, making the problem numerically tractable.

There is a useful connection between ridge regression and the SVD pseudo-inverse via ``filter factors.''  If
\begin{equation}
    A = \sum_{i=1}^r \sigma_i\, u_i v_i^T
\end{equation}
is the compact SVD, the truncated (pseudo-inverse) SVD solution with cutoff $\tau$ is
\begin{equation}
    x_{\mathrm{SVD}} = \sum_{i:\,\sigma_i>\tau} \frac{1}{\sigma_i}\,(u_i^T b)\, v_i.
\end{equation}
The ridge (Tikhonov) solution has the closed form
\begin{equation}
    x_{\mathrm{ridge}} = \sum_{i=1}^r \frac{\sigma_i}{\sigma_i^2+\lambda}\,(u_i^T b)\, v_i
    \;=\; \sum_{i=1}^r f_i(\lambda)\,\frac{u_i^T b}{\sigma_i}\, v_i,
\end{equation}
where the filter factor is
\begin{equation}
    f_i(\lambda) \;=\; \frac{\sigma_i^2}{\sigma_i^2+\lambda}\in(0,1).
\end{equation}
Thus ridge regression downweights small singular-value components smoothly (via $f_i(\lambda)$) instead of abruptly discarding them as in hard thresholding, making the solution less sensitive to the choice of cutoff.

\subsection{Experiment 4: Randomized SVD Parameter Sensitivity}

While not part of the original 1965 paper, randomized SVD algorithms have emerged as the method of choice for large-scale low-rank approximation. The seminal work of Halko et al.~\cite{halko2011} provides theoretical guarantees and practical heuristics, but empirical validation across different matrix structures remains important.

\subsubsection{Experimental Design}

We tested two $500 \times 500$ matrices with contrasting singular value decay:
\begin{itemize}
\item \textbf{Fast decay:} Geometric decay with $\kappa = 10^8$, simulating matrices with rapid spectral falloff (e.g., from smooth kernels).
\item \textbf{Slow decay:} Uniform decay with $\kappa = 100$, simulating matrices with gradual spectral falloff (e.g., from rough data).
\end{itemize}

For each matrix, we computed rank-20 approximations using randomized SVD with varying oversampling $p \in \{0, 5, 10, 20, 40\}$ and power iterations $q \in \{0, 1, 2, 3\}$. Each configuration was repeated 5 times to average out the randomness. We measured error relative to the best rank-20 approximation (computed via full SVD): $\|A_{\text{best}} - A_{\text{rand}}\|_F / \|A_{\text{best}}\|_F$.

\subsubsection{Results and Analysis}

Figure~\ref{fig:exp4} reveals dramatically different behavior for the two matrix types. For matrices with fast singular value decay (left panel), both oversampling and power iterations provide substantial improvements. With no oversampling ($p=0$) and no power iterations ($q=0$), the error is 63.4\%. Adding just $p=10$ reduces this to 50\%, while $q=3$ further reduces it to 24\%. The optimal configuration $p=40, q=3$ achieves $7.61 \times 10^{-6}$ error, essentially matching the best rank-20 approximation.

\begin{figure}[t]
\centering
\includegraphics[width=\textwidth]{../imgs/svd/exp4_randomized_svd.pdf}
\caption{Parameter sensitivity of randomized SVD for matrices with different spectral decay patterns. Left: Fast decay case showing dramatic improvement from both oversampling and power iterations. Right: Slow decay case exhibiting limited benefit from oversampling, with power iterations providing the primary accuracy gain.}
\label{fig:exp4}
\end{figure}

The fast decay case benefits from oversampling because the singular values drop off rapidly: $\sigma_{21}$ through $\sigma_{40}$ are already very small, so including them in the random sketch (via oversampling) allows the algorithm to capture the dominant $k=20$ dimensional subspace accurately. Power iterations $Y \leftarrow A(A^T Y)$ further enhance this by suppressing components in directions corresponding to small singular values, effectively performing $q$ iterations of simultaneous power iteration.

For matrices with slow singular value decay (right panel), the behavior is strikingly different. Oversampling provides minimal benefit: at $q=0$, increasing $p$ from 0 to 40 only reduces error from 118\% to 115\%. However, power iterations are highly effective: at $p=40$, increasing $q$ from 0 to 3 reduces error from 115\% to 75\%. The explanation is that with slow decay, $\sigma_{21}$ through $\sigma_{60}$ are all of comparable magnitude, so simply including more random vectors doesn't help much—they don't preferentially capture the top 20 directions. Power iterations, however, explicitly amplify the ratio $(\sigma_i/\sigma_j)^{2q}$ between large and small singular values, making the dominant subspace more distinguishable.

\subsection{Practical Recommendations}

These experiments suggest the following heuristics:
\begin{enumerate}
\item For matrices with fast spectral decay (e.g., arising from smooth operators), moderate oversampling ($p \approx k/2$) and one or two power iterations suffice.
\item For matrices with slow spectral decay (e.g., arising from discretized PDEs with rough coefficients), power iterations are essential; use $q \geq 2$ even if oversampling is limited.
\item The cost of power iterations is $2q$ matrix-vector products with $A$ and $A^T$. For large sparse $A$, this is often acceptable. For dense $A$, balance the $O(mnk)$ cost per power iteration against the required accuracy.
\end{enumerate}

The theoretical analysis of Halko et al.~predicts error bounds of the form
\begin{equation}
\mathbb{E}\|A - A_k\|_F \leq \left(1 + \frac{k}{p-1}\right)^{1/2} \left(\sum_{i>k} \sigma_i^2\right)^{1/2},
\end{equation}
for the basic algorithm, with power iterations improving the bound to involve $\sigma_i^{2q+1}$ instead of $\sigma_i$. Our experiments confirm this qualitative behavior but also reveal that the constants in these bounds can vary significantly with spectral structure.

\section{Conclusions and Future Directions}

Our experimental study has validated the key claims of the Golub-Kahan paper while extending the analysis to modern algorithmic variants. We have demonstrated that:
\begin{enumerate}
\item The Householder bidiagonalization algorithm can be implemented to achieve reconstruction accuracy at machine precision, with orthogonality errors bounded by $O(n \epsilon_{\text{mach}})$ as predicted by backward error analysis.
\item The choice of method for extracting singular values from the bidiagonal form has profound consequences for ill-conditioned matrices. The $2n \times 2n$ method maintains $O(\kappa \epsilon_{\text{mach}})$ accuracy in small singular values, while the $J^TJ$ method suffers $O(\kappa^2 \epsilon_{\text{mach}})$ degradation.
\item SVD-based pseudo-inverse computation is dramatically superior to normal equations for rank-deficient matrices, correctly identifying the rank and producing minimum-norm solutions, whereas normal equations can fail catastrophically.
\item Randomized SVD parameter tuning depends critically on spectral structure: fast decay benefits from oversampling, while slow decay requires power iterations. The interaction between these parameters is complex and problem-dependent.
\end{enumerate}

Several directions for future work emerge from this study. First, a detailed analysis of the \emph{deflation} strategy mentioned in the Golub-Kahan paper would illuminate how to compute the singular vectors without loss of orthogonality. Second, comparative experiments with modern divide-and-conquer and QR iteration methods would quantify the performance gap between pedagogical implementations and production-quality software. Third, extending the randomized SVD analysis to block Krylov methods and sketch-and-solve techniques would connect these experiments to current research in randomized numerical linear algebra.

The enduring relevance of the Golub-Kahan algorithm, nearly sixty years after its publication, testifies to the fundamental insight that orthogonal transformations provide a numerically stable path through the wilderness of ill-conditioned linear algebra. Our experiments confirm that this insight, when implemented carefully, delivers on its theoretical promises.

\bibliographystyle{plain}
\bibliography{refs}
\end{document}